# -*- coding: utf-8 -*-
"""Priceproject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qDEbbdbaZKfYdiYXp0CK4GzPYTESemxj

#Title: Laptop Price Prediction using Machine Learning

**1. Problem Statement**

Predict the price of laptops based on various features such as brand, processor, RAM, storage, screen size, and operating system. This can help consumers make informed decisions and vendors adjust pricing strategies.

**2. Outline of the Project (Steps)**

Load and inspect data

Clean and preprocess data

Feature engineering (optional)

Encoding categorical variables

Split data into training/testing

Train models

Predict and evaluate

Fine-tune using hyperparameters

Compare results and conclude

**3. Feature Scaling (Optional)**

Apply StandardScaler or MinMaxScaler to normalize continuous variables like:

Inches

Weight

Ram

Price_euros (target variable; only during analysis, not during training)

**4.Encoding**
Convert categorical columns like

Company

TypeName

OpSys

Cpu

Gpu
Using:

Label Encoding (if ordinal)

One Hot Encoding (if nominal)

5.**Model Preparation**

Split data into features and target

Train/Test Split using train_test_split()

Train models: Linear Regression, Random Forest,XBGregressor etc.

**6. Model Prediction**

Use .predict() to get predictions on test data.

**7. Model Evaluation Metrics**
Use:

MAE (Mean Absolute Error)

MSE (Mean Squared Error)

RMSE (Root Mean Squared Error)

R² Score (Accuracy)

**8. Hyperparameter Tuning**
Use:

GridSearchCV / RandomizedSearchCV
To tune models like  Random Forest

**9. Conclusion**
Describe how the model performed

Compare different algorithms

Choose the best-fit model based on metrics

Interpret model behavior
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')

df1 = pd.read_csv("/content/drive/MyDrive/DataSets/laptop_price.csv",encoding='latin-1')
df1

"""#Copy a data"""

df = df1.copy()
df

df.size

df.shape

#Basic Info and Data Types
df.info()

#Summary Statistics
df.describe()

#Checking for missing values
df.isnull().sum()

#4. Data Types and Conversion
#Check if Ram, Weight columns are strings (like "8GB", "1.37kg"). We will convert them.
# Convert 'Ram' and 'Weight' to numeric
df['Ram'] = df['Ram'].str.replace('GB', '').astype(int)
df['Weight'] = df['Weight'].str.replace('kg', '').astype(float)

#Unique Values in Categorical Columns
for col in ['Company', 'TypeName', 'OpSys', 'Cpu', 'Gpu']:
    print(f"{col} --> {df[col].nunique()} unique values")

for column in df.select_dtypes(include=["number"]).columns:
  plt.figure(figsize=(6,5))
  sns.boxplot(y=df[column])

cols=[ 'Ram','Weight','Inches']

numeric_cols=df.select_dtypes(include='number').columns

for cols in df.select_dtypes(include='number').columns:
  q1=df[cols].quantile(0.25)
  q3=df[cols].quantile(0.75)
  iqr=q3-q1
  lower_bound=q1-1.15*iqr
  upper_bound=q3+1.15*iqr
  df[cols] = df[cols].clip(lower=lower_bound, upper=upper_bound)

for column in df.select_dtypes(include=["number"]).columns:
  plt.figure(figsize=(6,5))
  sns.boxplot(y=df[column])

#Correlation Heatmap
#Shows which features are correlated with Price_euros.
plt.figure(figsize=(12, 6))
# Select only numeric columns for correlation calculation
numeric_df = df.select_dtypes(include=np.number)
sns.heatmap(numeric_df.corr(), annot=True, cmap='coolwarm')
plt.title("Correlation Heatmap")
plt.show()

#Visualizing Distributions
#Check the distribution of the target variable and features:
# Target variable
sns.histplot(df['Price_euros'], kde=True)
plt.title("Price Distribution")
plt.show()

# Company count
sns.countplot(y='Company', data=df)
plt.title("Laptop Brands Distribution")
plt.show()

"""#Feature Engineering"""

import re
#Extact CPU Brand
def get_cpu_type(cpu):
    cpu = cpu.lower()
    if 'intel' in cpu:
        if 'i3' in cpu:
            return 'Intel i3'
        elif 'i5' in cpu:
            return 'Intel i5'
        elif 'i7' in cpu:
            return 'Intel i7'
        elif 'i9' in cpu:
            return 'Intel i9'
        elif 'pentium' in cpu:
            return 'Intel Pentium'
        elif 'celeron' in cpu:
            return 'Intel Celeron'
        elif 'atom' in cpu:
            return 'Intel Atom'
        else:
            return 'Intel Other'
    elif 'amd' in cpu:
        if 'ryzen 3' in cpu:
            return 'AMD Ryzen 3'
        elif 'ryzen 5' in cpu:
            return 'AMD Ryzen 5'
        elif 'ryzen 7' in cpu:
            return 'AMD Ryzen 7'
        elif 'a9' in cpu:
            return 'AMD A9'
        elif 'a10' in cpu:
            return 'AMD A10'
        elif 'e2' in cpu:
            return 'AMD E2'
        else:
            return 'AMD Other'
    else:
        return 'Other'
df['Cpu_Brand'] = df['Cpu'].apply(get_cpu_type)
def extract_cpu_speed(cpu_str):
    match = re.search(r"(\d+(\.\d+)?)\s*GHz", cpu_str)
    return float(match.group(1)) if match else None

df['Cpu_Speed_GHz'] = df['Cpu'].apply(extract_cpu_speed)

#---Splits the 'Cpu' string and takes the first word (e.g., “Intel Core i5” → “Intel”).Creates a new column: Cpu_Brand.
#Identify Touchscreen laptop
df['Touchscreen'] = df['ScreenResolution'].apply(lambda x: 1 if 'Touchscreen' in x else 0)
#---Checks if 'Touchscreen' appears in the 'ScreenResolution' text. If yes, assigns 1; else 0.
#Identify IPS Display
df['IPS'] = df['ScreenResolution'].apply(lambda x: 1 if 'IPS' in x else 0)
#---Same as above, but for detecting whether the display is IPS technology.
#Extract Screen resolution
def extract_resolution(s): match = re.search(r'(\d+)x(\d+)', s); return (int(match.group(1)), int(match.group(2))) if match else (0, 0)
df['Resolution'] = df['ScreenResolution'].apply(extract_resolution)
df['X_res'] = df['Resolution'].apply(lambda x: x[0])
df['Y_res'] = df['Resolution'].apply(lambda x: x[1])
#--Uses regular expressions to extract numbers like “1920x1080”.Splits them into X and Y resolution columns
#calculate ppi
df['PPI'] = ((df['X_res']**2 + df['Y_res']**2)**0.5 / df['Inches']).round(2)
#--Uses the Pythagorean Theorem to compute screen pixel density:√(X² + Y²) / Screen Inches.Higher PPI = sharper display
#extract Gpu brand
df['Gpu_Brand'] = df['Gpu'].apply(lambda x: x.split()[0])
#--Same idea as CPU: extracts the first word from GPU string (e.g., "Nvidia GeForce" → "Nvidia").
#split memory into types
def split_memory(mem):
    mem = mem.replace('GB', '').replace('TB', '000').split('+')
    total = {'HDD': 0, 'SSD': 0, 'Flash': 0, 'Hybrid': 0}
    for part in mem:
        if 'HDD' in part: total['HDD'] += int(re.findall(r'\d+', part)[0])
        elif 'SSD' in part: total['SSD'] += int(re.findall(r'\d+', part)[0])
        elif 'Flash' in part: total['Flash'] += int(re.findall(r'\d+', part)[0])
        elif 'Hybrid' in part: total['Hybrid'] += int(re.findall(r'\d+', part)[0])
    return pd.Series(total)
df[['HDD', 'SSD', 'Flash', 'Hybrid']] = df['Memory'].apply(split_memory)
#--Breaks complex entries like “256GB SSD + 1TB HDD” into:.HDD = 1000,SSD = 256,Flash & Hybrid = 0 if not present.This helps the model learn from exact memory type capacities.
#Drop unnecessary columns
#---Removes columns that:Aren’t useful for modeling (like laptop_ID),Have already been processed into new useful columns (like ScreenResolution → X_res, Y_res, PPI),Creates a clean DataFrame df_fe with only the needed features.
df_fe = df.drop(columns=['laptop_ID', 'Product', 'ScreenResolution', 'Cpu', 'Gpu', 'Memory', 'Resolution'])
df

df.info()

"""#Encoding

"""

# Apply One-Hot Encoding
categorical_columns = ['Company', 'TypeName', 'OpSys', 'Cpu_Brand', 'Gpu_Brand'
]
df_encoded = pd.get_dummies(df_fe, columns=categorical_columns, drop_first=True)

# Check the result
print(df_encoded.shape)
df_encoded.head()

"""#Feature Scaling"""

from sklearn.preprocessing import StandardScaler

# List of numerical features to scale
numerical_features = ['Inches', 'Ram', 'Weight', 'X_res', 'Y_res', 'PPI', 'HDD', 'SSD', 'Flash', 'Hybrid']

# Initialize scaler
scaler = StandardScaler()

# Apply scaling on a copy of the dataframe
df_scaled = df_encoded.copy()
df_scaled[numerical_features] = scaler.fit_transform(df_encoded[numerical_features])

# Now df_scaled is your scaled dataset ready for training
df

sns.histplot(df['Price_euros'], kde=True)

df['Price_euros'] = np.log(df['Price_euros'])

df = pd.get_dummies(df, columns=['Company', 'TypeName', 'OpSys', 'Cpu_Brand', 'Gpu_Brand'], drop_first=True)

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
numeric_features = ['Ram', 'Weight', 'Inches', 'X_res', 'Y_res', 'PPI', 'HDD', 'SSD', 'Flash', 'Hybrid']
df[numeric_features] = scaler.fit_transform(df[numeric_features])

# The columns 'ScreenResolution', 'Memory', 'Cpu', 'Gpu', and 'Resolution' were dropped in a previous step.
# I will remove them from the list of columns to drop.
df = df.drop(columns=['laptop_ID', 'Product'])
df

from google.colab import drive
drive.mount('/content/drive')

"""#LinearRegression"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np

X = df_encoded.drop('Price_euros', axis=1)
y = df_encoded['Price_euros']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = LinearRegression()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

print("MAE:", round(mae, 2))
print("MSE:", round(mse, 2))
print("RMSE:", round(rmse, 2))
print("R² Score:", round(r2 * 100, 2), "%")

"""#RandomForestRegressor"""

from sklearn.ensemble import RandomForestRegressor
#create a model
rf_model = RandomForestRegressor(n_estimators=200, max_depth=15, random_state=40)
#train the model
rf_model.fit(X_train, y_train)
#make predications
y_pred_rf = rf_model.predict(X_test)
#Evalute the model
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np

mae = mean_absolute_error(y_test, y_pred_rf)
mse = mean_squared_error(y_test, y_pred_rf)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred_rf)

print("Random Forest Evaluation:")
print("MAE:", mae)
print("MSE:", mse)
print("RMSE:", rmse)
print("R² Score:", r2)

"""#RFR Hyperparamters"""

from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestRegressor

# Define parameter grid
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2],
    'max_features': ['auto', 'sqrt'],
    'bootstrap': [True, False]
}

# Initialize model
rf = RandomForestRegressor(random_state=42)

# Grid Search
grid_search = GridSearchCV(estimator=rf,param_grid=param_grid,cv=3,n_jobs=-1,verbose=2,scoring='r2')

# Fit to training data
grid_search.fit(X_train, y_train)

# Best model
print("✅ Best Parameters:", grid_search.best_params_)
mae = mean_absolute_error(y_test, y_pred_rf)
mse = mean_squared_error(y_test, y_pred_rf)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred_rf)

print("Random Forest Evaluation:")
print("MAE:", mae)
print("MSE:", mse)
print("RMSE:", rmse)
print("R² Score:", r2)

"""#XGBRegressor"""

from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np

# Clean up column names for XGBoost
X_train.columns = X_train.columns.str.replace('[', '_').str.replace(']', '_').str.replace('<', '_')
X_test.columns = X_test.columns.str.replace('[', '_').str.replace(']', '_').str.replace('<', '_')


xgb = XGBRegressor(n_estimators=300,learning_rate=0.2,max_depth=4,random_state=42)

xgb.fit(X_train, y_train)
y_pred_xgb = xgb.predict(X_test)

mae = mean_absolute_error(y_test, y_pred_xgb)
mse = mean_squared_error(y_test, y_pred_xgb)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred_xgb)

print("XGBoost Evaluation:")
print("MAE:", mae)
print("MSE:", mse)
print("RMSE:", rmse)
print("R² Score:", r2)

from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import r2_score

model = GradientBoostingRegressor(n_estimators=300, learning_rate=0.2, max_depth=4, random_state=42)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)
train_score = r2_score(y_train, model.predict(X_train))

print("MAE:", round(mae, 2))
print("MSE:", round(mse, 2))
print("RMSE:", round(rmse, 2))
print("R² Score:", round(r2 * 100, 2), "%")
print("Train R² Score:", round(train_score * 100, 2), "%")

from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
import numpy as np

# Use smaller learning rate and more estimators to reduce overfitting
model = GradientBoostingRegressor(
    n_estimators=400,
    learning_rate=0.05,
    max_depth=3,
    min_samples_split=4,
    min_samples_leaf=2,
    subsample=0.9,
    max_features='sqrt',
    random_state=42
)

model.fit(X_train, y_train)
y_pred = model.predict(X_test)

# Metrics
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)
train_score = r2_score(y_train, model.predict(X_train))

# Print scores
print("MAE:", round(mae, 2))
print("MSE:", round(mse, 2))
print("RMSE:", round(rmse, 2))
print("Test R² Score:", round(r2 * 100, 2), "%")
print("Train R² Score:", round(train_score * 100, 2), "%")

from sklearn.model_selection import cross_val_score
from sklearn.ensemble import GradientBoostingRegressor

model = GradientBoostingRegressor()

# Perform 5-fold cross-validation on your model
scores = cross_val_score(model, X, y, cv=5, scoring='r2')  # R² Score

print("Cross-validation scores (R²):", scores)
print("Average R² Score:", round(scores.mean() * 100, 2), "%")

from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np

model = XGBRegressor(
    n_estimators=5000,
    learning_rate=0.01,
    max_depth=15,
    subsample=0.8,
    colsample_bytree=0.8,
    reg_alpha=1,
    reg_lambda=1.5,
    random_state=40,
    n_jobs=-1,
    max_features= 8,
    min_samples_split= 8
)

model.fit(X_train, y_train)
y_pred = model.predict(X_test)

# Metrics
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)
train_score = r2_score(y_train, model.predict(X_train))

# Print results
print("MAE:", round(mae, 2))
print("MSE:", round(mse, 2))
print("RMSE:", round(rmse, 2))
print("Test R² Score:", round(r2 * 100, 2), "%")
print("Train R² Score:", round(train_score * 100, 2), "%")

